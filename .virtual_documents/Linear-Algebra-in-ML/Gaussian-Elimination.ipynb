# Gaussian Elimination


# Gaussian Elimination is like a step-by-step method to solve a system of linear equations. It's kind of like simplifying a puzzle by getting rid of one piece at a time until you find the solution.

# Imagine you have a few equations with variables (like x, y, z), and you want to find the values of those variables that make all the equations true at the same time. Gaussian Elimination helps you turn these equations into a simpler form, like a ladder, where the bottom equation has only numbers and the top ones have fewer variables.

# You start by picking an equation and changing it so that one variable has a coefficient of 1. Then, you use that equation to get rid of the same variable in the other equations. You keep doing this for all variables until you're left with a set of equations that are easy to solve. And voila! You've solved the system of equations and found the values of the variables.

# It's a bit like breaking down a big problem into smaller, manageable steps to find the answer.


import numpy as np

# Coefficient matrix
A = np.array([[2.0, 1.0, -1.0],
              [-3.0, -1.0, 2.0],
              [-2.0, 1.0, 2.0]])

# Constants column
b = np.array([8.0, -11.0, -3.0])

# Augmented matrix [A | b]
augmented_matrix = np.column_stack((A, b))

# Perform Gaussian Elimination
num_rows, num_cols = augmented_matrix.shape
for col in range(num_cols - 1):
    for row in range(col + 1, num_rows):
        factor = augmented_matrix[row, col] / augmented_matrix[col, col]
        augmented_matrix[row, col:] -= factor * augmented_matrix[col, col:]

# Back-substitution to find solutions
solutions = np.zeros(num_cols - 1)
for row in range(num_rows - 1, -1, -1):
    solutions[row] = (augmented_matrix[row, -1] - np.dot(augmented_matrix[row, row+1:num_cols-1], solutions[row+1:])) / augmented_matrix[row, row]

print("Solutions:", solutions)


# In this example, we start by defining the coefficient matrix `A` and the constants column `b`. We create an augmented matrix by combining `A` and `b`. Then, we perform Gaussian Elimination by applying row operations to transform the augmented matrix into an upper triangular form.

# After Gaussian Elimination, we use back-substitution to find the solutions for the variables. The solutions are printed as the final result.


# Inverse Matrix:


# An inverse matrix is like a "counterpart" matrix that, when multiplied with another matrix, gives you the identity matrix. The identity matrix is like the "do nothing" matrix, similar to the number 1 in multiplication. If you have a matrix A and you can find its inverse matrix A^-1, then A * A^-1 = I (identity matrix).

# Inverse matrices are useful for solving equations and transforming matrices back to their original forms after certain operations.


# Here's a very simple Python example demonstrating Gaussian Elimination and finding the inverse matrix using NumPy:


import numpy as np

# Coefficient matrix
A = np.array([[2, 1],
              [1, 3]])

# Constants column
b = np.array([8, 11])

# Solve using Gaussian Elimination
solution = np.linalg.solve(A, b)
print("Solution using Gaussian Elimination:", solution)

# Find the inverse matrix
A_inverse = np.linalg.inv(A)
print("Inverse matrix:", A_inverse)


# In this example, we have a 2x2 coefficient matrix `A` and a constants column `b`. We use `np.linalg.solve()` to find the solution using Gaussian Elimination. Then, we use `np.linalg.inv()` to find the inverse matrix of `A`. Keep in mind that not all matrices have inverses, so be cautious when using this technique.


# The following is another example:


A = np.array([[1, 2],
              [3, 4]])
A


Ainv = np.linalg.inv(A)
Ainv


b = np.array([5, 11])
b


x = np.dot(Ainv, b)
x


# Check result


np.dot(A, x)


# Inverse and Determinant:


# Inverse:
# The inverse of a matrix is like its "undo" button. When you multiply a matrix by its inverse, you get the identity matrix, which is like the number 1 for matrices. It's used to "reverse" operations done with the matrix. Imagine you've applied transformations to your data using a matrix, and you want to get back to the original data. The inverse matrix helps you do that.

# Determinant:
# The determinant of a matrix is like a special number that tells you about the "scaling factor" of the matrix. It's often used to understand how much an area or volume changes when you transform it using the matrix. A determinant of 0 means the matrix doesn't change the area or volume at all, like squishing everything down to a line or a point.


import numpy as np

# Example matrix
A = np.array([[2, 1],
              [1, 3]])

# Calculate the determinant
determinant = np.linalg.det(A)
print("Determinant of the matrix:", determinant)

# Calculate the inverse matrix
A_inverse = np.linalg.inv(A)
print("Inverse matrix:")
print(A_inverse)


# In this example, we have a 2x2 matrix `A`. We use `np.linalg.det()` to calculate its determinant and `np.linalg.inv()` to calculate the inverse matrix. Please note that not all matrices have inverses, and the determinant can help determine if a matrix has one.


# The following is another example:


A = np.array([[1, 2],
              [3, 4]])
A


det = np.linalg.det(A)
det


B = np.array([[3, 1],
              [6, 2]])
B


np.linalg.det(B)


np.linalg.inv(B)


# It is not possible to calculate the inverse of a singular matrix.
